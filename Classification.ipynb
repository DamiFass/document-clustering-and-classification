{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd20ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "843bafc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 128 images belonging to 3 classes.\n",
      "Found 21 images belonging to 3 classes.\n",
      "Found 50 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 8\n",
    "\n",
    "# Implement the necessary pre-processing for our image to work with\n",
    "# the pre-trained VGG\n",
    "train_generator = ImageDataGenerator(rotation_range=90, \n",
    "                                     brightness_range=[0.1, 0.7],\n",
    "                                     width_shift_range=0.5, \n",
    "                                     height_shift_range=0.5,\n",
    "                                     horizontal_flip=True, \n",
    "                                     vertical_flip=True,\n",
    "                                     validation_split=0.15,\n",
    "                                     preprocessing_function=preprocess_input) # VGG16 preprocessing\n",
    "\n",
    "test_generator = ImageDataGenerator(preprocessing_function=preprocess_input) # VGG16 preprocessing\n",
    "\n",
    "train_data_dir = 'documents-101_ward/training/'\n",
    "test_data_dir = 'documents-101_ward/test/'\n",
    "\n",
    "# I named the 3 classes identified by the clustering in this way.\n",
    "# The class \"others\" is more of a miscellanea class (it could have been\n",
    "# divided into further classes)\n",
    "class_subset = ['others','cost_authorization','coupon_registration']\n",
    "\n",
    "traingen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               subset='training',\n",
    "                                               batch_size=BATCH_SIZE, \n",
    "                                               shuffle=False,\n",
    "                                               seed=42)\n",
    "\n",
    "validgen = train_generator.flow_from_directory(train_data_dir,\n",
    "                                               target_size=(224, 224),\n",
    "                                               class_mode='categorical',\n",
    "                                               classes=class_subset,\n",
    "                                               subset='validation',\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               seed=42)\n",
    "\n",
    "testgen = test_generator.flow_from_directory(test_data_dir,\n",
    "                                             target_size=(224, 224),\n",
    "                                             class_mode=None,\n",
    "                                             classes=class_subset,\n",
    "                                             batch_size=1,\n",
    "                                             shuffle=False,\n",
    "                                             seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d889a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model using the pre-trained VGG16\n",
    "def create_model(input_shape, n_classes, optimizer='rmsprop', fine_tune=0):\n",
    "\n",
    "    # Load the convolutional layer pre-trained on the ImageNet data,\n",
    "    # but not the fully-connected layers.\n",
    "    conv_base = VGG16(include_top=False,\n",
    "                     weights='imagenet', \n",
    "                     input_shape=input_shape)\n",
    "    \n",
    "    # Layers to freeze during training: \n",
    "    if fine_tune > 0:\n",
    "        for layer in conv_base.layers[:-fine_tune]:\n",
    "            layer.trainable = False\n",
    "    else:\n",
    "        for layer in conv_base.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "    # Create the new fully-connected layers\n",
    "    top_model = conv_base.output\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(4096, activation='relu')(top_model)\n",
    "    top_model = Dense(1072, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.2)(top_model)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "    \n",
    "    # Group into a Model object\n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "\n",
    "    # Compiles the model\n",
    "    model.compile(optimizer=optimizer, \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "845e0097",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-06 20:40:34.228615: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-06-06 20:40:34.228800: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-06 20:40:34.230880: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2022-06-06 20:40:35.083602: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-06-06 20:40:35.083989: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2592230000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 11s 594ms/step - loss: 73.6983 - accuracy: 0.8150 - val_loss: 6.3328 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 6.33277, saving model to tl_model_v1.weights.best.ward.hdf5\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 9s 545ms/step - loss: 6.6309 - accuracy: 0.6008 - val_loss: 7.4506e-08 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss improved from 6.33277 to 0.00000, saving model to tl_model_v1.weights.best.ward.hdf5\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 8s 523ms/step - loss: 4.0070 - accuracy: 0.8681 - val_loss: 0.6967 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 8s 511ms/step - loss: 2.0037 - accuracy: 0.8699 - val_loss: 0.5082 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 8s 512ms/step - loss: 2.1043 - accuracy: 0.9534 - val_loss: 0.8950 - val_accuracy: 0.5000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 8s 523ms/step - loss: 1.2073 - accuracy: 0.6828 - val_loss: 1.3959e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 9s 527ms/step - loss: 1.1301 - accuracy: 0.9283 - val_loss: 0.3469 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 9s 528ms/step - loss: 0.6394 - accuracy: 0.8693 - val_loss: 0.3951 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - 9s 528ms/step - loss: 0.7070 - accuracy: 0.7999 - val_loss: 0.2994 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.00000\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - 9s 531ms/step - loss: 1.1885 - accuracy: 0.9036 - val_loss: 0.8066 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.00000\n"
     ]
    }
   ],
   "source": [
    "# Define training parameters:\n",
    "input_shape = (224, 224, 3)\n",
    "optim_1 = Adam(learning_rate=0.001)\n",
    "n_classes=3\n",
    "\n",
    "n_steps = traingen.samples // BATCH_SIZE\n",
    "n_val_steps = validgen.samples // BATCH_SIZE\n",
    "n_epochs = 10\n",
    "\n",
    "# Create a model with all the pre-trained layers frozen:\n",
    "vgg_model = create_model(input_shape, n_classes, optim_1, fine_tune=0)\n",
    "\n",
    "# Import to live plot the \n",
    "from livelossplot.inputs.keras import PlotLossesCallback\n",
    "\n",
    "plot_loss_1 = PlotLossesCallback()\n",
    "\n",
    "# Checkpoints to save best weights\n",
    "tl_checkpoint_1 = ModelCheckpoint(filepath='tl_model_v1.weights.best.ward.hdf5',\n",
    "                                  save_best_only=True,\n",
    "                                  verbose=1)\n",
    "\n",
    "# Early stopping criteria\n",
    "early_stop = EarlyStopping(monitor='val_loss',\n",
    "                           patience=10,\n",
    "                           restore_best_weights=True,\n",
    "                           mode='min')\n",
    "\n",
    "# Model training: in this case we will train only the new fully-connected\n",
    "# layer for doing the prediction, the pre-trained layer will\n",
    "# perform the feature extraction:\n",
    "vgg_history = vgg_model.fit(traingen,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            epochs=n_epochs,\n",
    "                            validation_data=validgen,\n",
    "                            steps_per_epoch=n_steps,\n",
    "                            validation_steps=n_val_steps,\n",
    "                            callbacks=[tl_checkpoint_1, early_stop],\n",
    "                            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e9bb28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 70.00%\n"
     ]
    }
   ],
   "source": [
    "# Prediction:\n",
    "# Load the best trained weights:\n",
    "vgg_model.load_weights('tl_model_v1.weights.best.ward.hdf5') \n",
    "\n",
    "true_classes = testgen.classes\n",
    "class_indices = traingen.class_indices\n",
    "class_indices = dict((v,k) for k,v in class_indices.items())\n",
    "\n",
    "vgg_preds = vgg_model.predict(testgen)\n",
    "vgg_pred_classes = np.argmax(vgg_preds, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "vgg_acc = accuracy_score(true_classes, vgg_pred_classes)\n",
    "print(\"Classification accuracy: {:.2f}%\".format(vgg_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c3c09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/16 [==============================] - 11s 657ms/step - loss: 6.9139 - accuracy: 0.7523 - val_loss: 0.2838 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.00000\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - 10s 613ms/step - loss: 10.1536 - accuracy: 0.6672 - val_loss: 4.9059e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.00000\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - 10s 636ms/step - loss: 1.5598 - accuracy: 0.9144 - val_loss: 0.0542 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.00000\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - 10s 616ms/step - loss: 1.7406 - accuracy: 0.7732 - val_loss: 0.5010 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.00000\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - 10s 607ms/step - loss: 0.8791 - accuracy: 0.8693 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.00000\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - 10s 630ms/step - loss: 0.5468 - accuracy: 0.8927 - val_loss: 0.5395 - val_accuracy: 0.8750\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.00000\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - 10s 636ms/step - loss: 0.7892 - accuracy: 0.8360 - val_loss: 0.8695 - val_accuracy: 0.9375\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.00000\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - 10s 633ms/step - loss: 0.6132 - accuracy: 0.8878 - val_loss: 0.6560 - val_accuracy: 0.6875\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.00000\n",
      "Epoch 9/10\n",
      " 2/16 [==>...........................] - ETA: 8s - loss: 0.3092 - accuracy: 0.8125 "
     ]
    }
   ],
   "source": [
    "# Try again with the same model, but letting backrpopagation update\n",
    "# the weights of the last two layers:\n",
    "traingen.reset()\n",
    "validgen.reset()\n",
    "testgen.reset()\n",
    "\n",
    "# Smaller learning rate\n",
    "optim_2 = Adam(lr=0.0001)\n",
    "\n",
    "# Re-create the model with the parameter fine_tuning=2:\n",
    "vgg_model_ft = create_model(input_shape, n_classes, optim_2, fine_tune=2)\n",
    "\n",
    "plot_loss_2 = PlotLossesCallback()\n",
    "\n",
    "# Retrain the model:\n",
    "vgg_ft_history = vgg_model_ft.fit(traingen,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  epochs=n_epochs,\n",
    "                                  validation_data=validgen,\n",
    "                                  steps_per_epoch=n_steps, \n",
    "                                  validation_steps=n_val_steps,\n",
    "                                  callbacks=[tl_checkpoint_1, early_stop],\n",
    "                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb7383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction (once again):\n",
    "# Load the best trained weights:\n",
    "vgg_model_ft.load_weights('tl_model_v1.weights.best.ward.hdf5')\n",
    "\n",
    "vgg_preds_ft = vgg_model_ft.predict(testgen)\n",
    "vgg_pred_classes_ft = np.argmax(vgg_preds_ft, axis=1)\n",
    "vgg_acc_ft = accuracy_score(true_classes, vgg_pred_classes_ft)\n",
    "print(\"Classification accuracy with last 2 layers re-trained: {:.2f}%\".format(vgg_acc_ft * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e847fd6",
   "metadata": {},
   "source": [
    "**FINAL COMMENTS**\n",
    "\n",
    "The accuracy is not very high, and I think the main cause is the low number of data. The training set is 149 images and it seems that some classes only have around 10 images. Also, the network has been trained on recognising images that can be very different from each other (e.g. a car vs a dog). This task instead requires to distinguish between images that are all documents, so the difference between them is more subtle (I have tried just to feed them to the pre-trained VGG16 with the default 1000 classes and they were all classified as \"menus\").\n",
    "\n",
    "Also I could have probably defined at least a couple more classes in the clustering (4 or 5 classes in total). After doing the classification with the VGG net, I played around a bit more with the clustering, and I noticed that the classes defined by the agglomerative clustering when setting k=4 or 5 where actually breaking down the \"big\" class into smaller classes, which made sense, by checking the pictures manually.   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
