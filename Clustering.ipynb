{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries:\n",
    "import os\n",
    "import umap \n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from text_preprocessing import *\n",
    "import sklearn.cluster as cluster\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "# I initially used the annotations files to get the all the words from the each document:\n",
    "directory= 'dataset/training_data/annotations/'\n",
    "print('Found {} files for training in the given directory'.format(len(os.listdir(directory))))\n",
    "\n",
    "list_of_documents = []\n",
    "for filename in os.listdir(directory):\n",
    "\tf = os.path.join(directory, filename)\n",
    "\t# checking if it is a file\n",
    "\tif os.path.isfile(f):\n",
    "\t\tfile = open(f)\n",
    "\t\tdata = json.load(file)\n",
    "\t\tdocument = ''\n",
    "\t\tfor i in range(len(data['form'])):\n",
    "\t\t\tdocument = document + ' ' + data['form'][i]['text']\n",
    "\t\tlist_of_documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I noticed that the text extracted from the annotations files is quite\n",
    "# \"dirty\", so I did some text pre-processing to clean it:\n",
    "DOCS = []\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "for i in list_of_documents:\n",
    "\tcleaned = preprocess_text(i, [to_lower, remove_punctuation, remove_number, remove_whitespace, remove_stopword])\n",
    "\tDOCS.append(shortword.sub('', cleaned))\n",
    "# Example of comparison between pre and post-processed text:\n",
    "print(list_of_documents[10])\n",
    "print('\\n')\n",
    "print(DOCS[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize list of words:\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(DOCS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try: K-means clustering. We try for a range of different classes K \n",
    "# and we plot the sum of the squared distances of each sample to its cluster centre\n",
    "RSS = []\n",
    "for k in range(1,50):\n",
    "\tkm = cluster.KMeans(n_clusters=k, init=\"k-means++\", max_iter=100, n_init=20)\n",
    "\tkm.fit(X)\n",
    "\tRSS.append(km.inertia_)\n",
    "# \tprint(metrics.silhouette_score(X, km.labels_, sample_size=149))\n",
    "\n",
    "sns.set()\n",
    "plt.plot(range(1,50),RSS,'bx-', linewidth=3)\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('RSS')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RSS decreases as the number of classes increases, as expected, but there is no \"elbow\" in the curve, i.e. we don't see a steep decrement of the RSS within low numbers of classes. So it seems K-means is failing to cluster the data.\n",
    "\n",
    "In light of this, I tried to see if hierarchical clustering could perform better. I tried different linkages and affinity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the dendogram:\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through the different affinity metrics and linkages:\n",
    "for ind, metric in enumerate([\"cosine\", \"euclidean\", \"cityblock\"]):\n",
    "    for ind, link in enumerate([\"complete\", \"average\", \"single\"]):\n",
    "        model = AgglomerativeClustering(n_clusters=None, distance_threshold=0, compute_full_tree=True, linkage=link, affinity=metric)\n",
    "        model.fit(X.toarray())\n",
    "        fig, ax = plt.subplots()\n",
    "        plot_dendrogram(model, truncate_mode=\"level\", p=3)\n",
    "        ax.set_title(\"Linkage = {}    Metric = {}\".format(link,metric))\n",
    "        ax.set_xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "        fig.show()\n",
    "# Try also ward linkage, which only accepts euclidean as metric:\n",
    "metric = \"euclidean\"\n",
    "link = \"ward\"\n",
    "model = AgglomerativeClustering(n_clusters=None, distance_threshold=0, compute_full_tree=True, linkage=link, affinity=metric)\n",
    "model.fit(X.toarray())\n",
    "fig, ax = plt.subplots()\n",
    "plot_dendrogram(model, truncate_mode=\"level\", p=3)\n",
    "ax.set_title(\"Linkage = {}    Metric = {}\".format(link,metric))\n",
    "ax.set_xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "fig.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By inspecting the dendogram plots I was not able to see a clear number of clusters.\n",
    "\n",
    "I have then decided to reduce the dimensionality of the vectorized text in order to be able to plot and visualize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA:\n",
    "pca = PCA(n_components=2)\n",
    "two_dim = pca.fit_transform(X.toarray())\n",
    "scatter_x = two_dim[:, 0] # first principle component\n",
    "scatter_y = two_dim[:, 1] # second principle component\n",
    "# Plot 2D dataset:\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(scatter_x, scatter_y)\n",
    "plt.xlabel(\"PCA First component\")\n",
    "plt.ylabel(\"PCA Second component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the scatterplot the clusters in our data are not clearly visible, expect for 2 small clusters on the bottom right and top left. Given this information, I have decided to re-run the hierarchical clustering, this tipe specifiying a the number of classes to be 3.\n",
    "\n",
    "For each metric and linkage I replicate the scatterplot to see whether the clustering algorithm has been able to distinguish the 3 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate again through the different affinity metrics and linkages:\n",
    "for ind, metric in enumerate([\"cosine\", \"euclidean\", \"cityblock\"]):\n",
    "    for ind, link in enumerate([\"complete\", \"average\", \"single\"]):\n",
    "        model = AgglomerativeClustering(n_clusters=3, distance_threshold=None, compute_full_tree=True, linkage=link, affinity=metric)\n",
    "        model.fit(X.toarray())\n",
    "        clusters = model.fit_predict(X.toarray())\n",
    "        fig, ax = plt.subplots()\n",
    "        cmap = {0: 'yellow', 1: 'blue', 2: 'red'}\n",
    "        # scatter every cluster with a different colour:\n",
    "        for group in np.unique(clusters):\n",
    "            ix = np.where(clusters == group)\n",
    "            ax.scatter(scatter_x[ix], scatter_y[ix], c=cmap[group], label=group)\n",
    "            ax.set_title(\"Linkage = {}    Metric = {}\".format(link,metric))\n",
    "        plt.xlabel(\"PCA First component\")\n",
    "        plt.ylabel(\"PCA Second component\")\n",
    "\n",
    "# Try also ward linkage, which only accepts euclidean as metric:        \n",
    "metric = \"euclidean\"\n",
    "link = \"ward\"\n",
    "model = AgglomerativeClustering(n_clusters=3, distance_threshold=None, compute_full_tree=True, linkage=link, affinity=metric)\n",
    "model.fit(X.toarray())\n",
    "clusters = model.fit_predict(X.toarray())\n",
    "fig, ax = plt.subplots()\n",
    "cmap = {0: 'yellow', 1: 'blue', 2: 'red'}\n",
    "# scatter every cluster with a different colour:\n",
    "for group in np.unique(clusters):\n",
    "    ix = np.where(clusters == group)\n",
    "    ax.scatter(scatter_x[ix], scatter_y[ix], c=cmap[group], label=group)\n",
    "ax.set_title(\"Linkage = {}    Metric = {}\".format(link,metric))\n",
    "plt.xlabel(\"PCA First component\")\n",
    "plt.ylabel(\"PCA Second component\")\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As visible from the last graph, the ward linkage with an euclidean metric is able to distinguish the 3 clusters.\n",
    "\n",
    "As a **classifier**, I wanted to use the pre-trained VGG net. For that, I need labels for the images in the test data, so I can evaluate the performance of the classifier. I also organized the training set with the newly found label, so that they could come handy to partially re-train the VGG net (i.e., do transfer learning).\n",
    "\n",
    "I could not manually label the images in the test data, so I classified them with the clustering model learned before (linkage ward + euclidean metric) and scatter-plotted them against the training data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test data:\n",
    "directory= 'dataset/testing_data/annotations/'\n",
    "print('Found {} files in the given directory'.format(len(os.listdir(directory))))\n",
    "\n",
    "list_of_documents = []\n",
    "# iterate over files in that directory\n",
    "for filename in os.listdir(directory):\n",
    "    f = os.path.join(directory, filename)\n",
    "    # checking if it is a file\n",
    "    if os.path.isfile(f):\n",
    "        file = open(f)\n",
    "        data = json.load(file)\n",
    "        document = ''\n",
    "        for i in range(len(data['form'])):\n",
    "            document = document + ' ' + data['form'][i]['text']\n",
    "        list_of_documents.append(document)\n",
    "\n",
    "DOCS = []\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "\n",
    "for i in list_of_documents:\n",
    "    cleaned = preprocess_text(i, [to_lower, remove_punctuation, remove_number, remove_whitespace, remove_stopword])\n",
    "    DOCS.append(shortword.sub('', cleaned))\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=3, stop_words='english')\n",
    "X = tfidf_vectorizer.fit_transform(DOCS)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "two_dim = pca.fit_transform(X.toarray())\n",
    "scatter_x2 = two_dim[:, 0] # first principle component\n",
    "scatter_y2 = two_dim[:, 1] # second principle component\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(scatter_x, scatter_y, label='training')\n",
    "ax.scatter(scatter_x2, scatter_y2, c='black', label='test')\n",
    "plt.xlabel(\"PCA First component\")\n",
    "plt.ylabel(\"PCA Second component\")\n",
    "ax.legend()\n",
    "plt.title('Training data + test data (in black)')\n",
    "\n",
    "\n",
    "for ind, metric in enumerate([\"euclidean\"]):\n",
    "    for ind, link in enumerate([\"ward\"]):\n",
    "        model = AgglomerativeClustering(n_clusters=3, distance_threshold=None, compute_full_tree=True, linkage=link, affinity=metric)\n",
    "        model.fit(X.toarray())\n",
    "        clusters = model.fit_predict(X.toarray())\n",
    "#         pca = PCA(n_components=2)\n",
    "#         two_dim = pca.fit_transform(X.toarray())\n",
    "#         scatter_x = two_dim[:, 0] # first principle component\n",
    "#         scatter_y = two_dim[:, 1] # second principle component\n",
    "        fig, ax = plt.subplots()\n",
    "        # scatter every cluster with a different colour:\n",
    "        cmap = {0: 'yellow', 1: 'blue', 2: 'red'}\n",
    "        for group in np.unique(clusters):\n",
    "            ix = np.where(clusters == group)\n",
    "            ax.scatter(scatter_x2[ix], scatter_y2[ix], c=cmap[group], label=group)\n",
    "        ax.legend()\n",
    "        plt.xlabel(\"PCA First component\")\n",
    "        plt.ylabel(\"PCA Second component\")\n",
    "        plt.title('Predictions on the test data')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Classification using VGG net is in the other notebook file)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
